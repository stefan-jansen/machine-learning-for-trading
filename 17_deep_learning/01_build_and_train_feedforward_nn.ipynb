{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use backpropagation to train a feedforward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This noteboo implements a simple single-layer architecture and forward propagation computations using matrix algebra and Numpy,the Python counterpart of linear algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the installations [instructions](../installation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.017606Z",
     "start_time": "2020-03-14T17:39:22.015888Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.738769Z",
     "start_time": "2020-03-14T17:39:22.020627Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D plots\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.741953Z",
     "start_time": "2020-03-14T17:39:22.739775Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotting style\n",
    "sns.set_style('whitegrid')\n",
    "# for reproducibility\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target `y` represents two classes generated by two circular distribution that are not linearly separable because class 0 surrounds class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate 50,000 random samples in the form of two concentric circles with different radius using scikit-learn's make_circles function so that the classes are not linearly separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.756451Z",
     "start_time": "2020-03-14T17:39:22.743000Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset params\n",
    "N = 50000\n",
    "factor = 0.1\n",
    "noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.763936Z",
     "start_time": "2020-03-14T17:39:22.757397Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iterations = 50000\n",
    "learning_rate = 0.0001\n",
    "momentum_factor = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.778511Z",
     "start_time": "2020-03-14T17:39:22.764841Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "X, y = make_circles(\n",
    "    n_samples=N,\n",
    "    shuffle=True,\n",
    "    factor=factor,\n",
    "    noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.782573Z",
     "start_time": "2020-03-14T17:39:22.779452Z"
    }
   },
   "outputs": [],
   "source": [
    "# define outcome matrix\n",
    "Y = np.zeros((N, 2))\n",
    "for c in [0, 1]:\n",
    "    Y[y == c, c] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X =\\begin{bmatrix} \n",
    "x_{11} & x_{12} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{N1} & x_{N2}\n",
    "\\end{bmatrix}\n",
    "\\quad\\quad\n",
    "Y = \\begin{bmatrix} \n",
    "y_{11} & y_{12}\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "y_{N1} & y_{N2} \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:22.795287Z",
     "start_time": "2020-03-14T17:39:22.783774Z"
    }
   },
   "outputs": [],
   "source": [
    "f'Shape of: X: {X.shape} | Y: {Y.shape} | y: {y.shape}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.504677Z",
     "start_time": "2020-03-14T17:39:22.797818Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X[:, 0], \n",
    "                y=X[:, 1], \n",
    "                hue=y,\n",
    "               style=y,\n",
    "               markers=['_', '+'])\n",
    "ax.set_title('Synthetic Classification Data')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/ffnn_data', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer $h$ projects the 2D input into a 3D space. To this end, the hidden layer weights are a $2\\times3$ matrix $\\mathbf{W}^h$, and the hidden layer bias vector $\\mathbf{b}^h$ is a 3-dimensional vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\underset{\\scriptscriptstyle 2 \\times 3}{\\mathbf{W}^h} =\n",
    "\\begin{bmatrix} \n",
    "w^h_{11} & w^h_{12} & w^h_{13} \\\\\n",
    "w^h_{21} & w^h_{22} & w^h_{23}\n",
    "\\end{bmatrix}\n",
    "&& \\underset{\\scriptscriptstyle 1 \\times 3}{\\mathbf{b}^h} = \n",
    "\\begin{bmatrix} \n",
    "b^h_1 & b^h_2 & b^h_3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer values $\\mathbf{Z}^h$ result from the dot product of the $N\\times\\ 2$ input data $\\mathbf{X}$ and the the $2\\times3$ weight matrix $\\mathbf{W}^h$ and the addition of the $1\\times3$ hidden layer bias vector $\\mathbf{b}^h$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\scriptscriptstyle N \\times 3}{\\mathbf{Z}^h} = \\underset{\\scriptscriptstyle N \\times 2}{\\vphantom{\\mathbf{W}^o}\\mathbf{X}}\\cdot\\underset{\\scriptscriptstyle 2 \\times 3}{\\mathbf{W}^h} + \\underset{\\scriptscriptstyle 1 \\times 3}{\\mathbf{b}^h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic sigmoid function $\\sigma$ applies a non-linear transformation to $\\mathbf{Z}^h$ to yield  the hidden layer activations as an $N\\times3$ matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\scriptscriptstyle N \\times 3}{\\mathbf{H}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{W}^h + \\mathbf{b}^h) = \\frac{1}{1+e^{−(\\mathbf{X} \\cdot \\mathbf{W}^h + \\mathbf{b}^h)}} = \\begin{bmatrix} \n",
    "h_{11} & h_{12} & h_{13} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "h_{N1} & h_{N2} & h_{N3}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.507896Z",
     "start_time": "2020-03-14T17:39:23.505786Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    \"\"\"Logistic function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.519238Z",
     "start_time": "2020-03-14T17:39:23.508805Z"
    }
   },
   "outputs": [],
   "source": [
    "def hidden_layer(input_data, weights, bias):\n",
    "    \"\"\"Compute hidden activations\"\"\"\n",
    "    return logistic(input_data @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values $\\mathbf{Z}^o$ for the output layer $o$ are a $N\\times2$ matrix that results from the dot product of the $\\underset{\\scriptscriptstyle N \\times 3}{\\mathbf{H}}$ hidden layer activation matrix with the $3\\times2$ output weight matrix $\\mathbf{W}^o$ and the addition of the $1\\times2$ output bias vector $\\mathbf{b}^o$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\scriptscriptstyle N \\times 2}{\\mathbf{Z}^o} = \\underset{\\scriptscriptstyle N \\times 3}{\\vphantom{\\mathbf{W}^o}\\mathbf{H}}\\cdot\\underset{\\scriptscriptstyle 3 \\times 2}{\\mathbf{W}^o} + \\underset{\\scriptscriptstyle 1 \\times 2}{\\mathbf{b}^o}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:39:58.307221Z",
     "start_time": "2019-01-10T13:39:58.295854Z"
    }
   },
   "source": [
    "The Softmax function $\\varsigma$ squashes the unnormalized probabilities predicted for each class  to lie within $[0, 1]$ and sum to 1.  The result is a $N\\times2$ matrix with one column for each output class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\scriptscriptstyle N \\times 2}{\\mathbf{\\hat{Y}}} \n",
    "= \\varsigma(\\mathbf{H} \\cdot \\mathbf{W}^o + \\mathbf{b}^o)\n",
    "= \\frac{e^{Z^o}}{\\sum_{c=1}^C e^{\\mathbf{z}^o_c}}\n",
    "= \\frac{e^{H \\cdot W^o + \\mathbf{b}^o}}{\\sum_{c=1}^C e^{H \\cdot \\mathbf{w}^o_c + b^o_c}}\n",
    "= \\begin{bmatrix} \n",
    "\\hat{y}_{11} & \\hat{y}_{12}\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\hat{y}_{n1} & \\hat{y}_{n2} \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.527595Z",
     "start_time": "2020-03-14T17:39:23.520274Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Softmax function\"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.536538Z",
     "start_time": "2020-03-14T17:39:23.528473Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_layer(hidden_activations, weights, bias):\n",
    "    \"\"\"Compute the output y_hat\"\"\"\n",
    "    return softmax(hidden_activations @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward_prop` function combines the previous operations to yield the output activations from the  input data as a function of weights and biases. The `predict` function produces the binary class predictions given weights, biases, and input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.549810Z",
     "start_time": "2020-03-14T17:39:23.537499Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop(data, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    \"\"\"Neural network as function.\"\"\"\n",
    "    hidden_activations = hidden_layer(data, hidden_weights, hidden_bias)\n",
    "    return output_layer(hidden_activations, output_weights, output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.558137Z",
     "start_time": "2020-03-14T17:39:23.550820Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(data, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    \"\"\"Predicts class 0 or 1\"\"\"\n",
    "    y_pred_proba = forward_prop(data,\n",
    "                                hidden_weights,\n",
    "                                hidden_bias,\n",
    "                                output_weights,\n",
    "                                output_bias)\n",
    "    return np.around(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function $J$ uses the cross-entropy loss $\\xi$ that sums the deviations of the predictions for each class $c$  $\\hat{y}_{ic}, i=1,...,N$ from the actual outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{Y},\\mathbf{\\hat{Y}}) = \\sum_{i=1}^n \\xi(\\mathbf{y}_i,\\mathbf{\\hat{y}}_i) = − \\sum_{i=1}^N \\sum_{i=c}^{C} y_{ic} \\cdot log(\\hat{y}_{ic})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.566808Z",
     "start_time": "2020-03-14T17:39:23.559024Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(y_hat, y_true):\n",
    "    \"\"\"Cross-entropy\"\"\"\n",
    "    return - (y_true * np.log(y_hat)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation updates parameters values based on the partial derivative of the loss with respect to that parameter, computed using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the loss function $J$ with respect to each output layer activation $\\varsigma(\\mathbf{Z}^o_i), i=1,...,N$, is a very simple expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial z^0_i} = \\delta^o = \\hat{y}_i-y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function) and [here](https://deepnotes.io/softmax-crossentropy) for details on derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.574632Z",
     "start_time": "2020-03-14T17:39:23.567768Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_gradient(y_hat, y_true):\n",
    "    \"\"\"output layer gradient\"\"\"\n",
    "    return y_hat - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Weight Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To propagate the updates back to the output layer weights, we take the partial derivative of the loss function with respect to the weight matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T04:18:37.478549Z",
     "start_time": "2019-01-08T04:18:37.476003Z"
    }
   },
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^o} = H^T \\cdot (\\mathbf{\\hat{Y}}-\\mathbf{Y}) = H^T \\cdot \\delta^{o}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.582859Z",
     "start_time": "2020-03-14T17:39:23.575489Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_weight_gradient(H, loss_grad):\n",
    "    \"\"\"Gradients for the output layer weights\"\"\"\n",
    "    return  H.T @ loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Bias Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the output layer bias values, we similarly apply the chain rule to obtain the partial derivative of the loss function with respect to the bias vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}_{o}} \n",
    "= \\frac{\\partial \\xi}{\\partial \\mathbf{\\hat{Y}}} \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}^o}  \\frac{\\partial \\mathbf{Z}^{o}}{\\partial \\mathbf{b}^o}\n",
    "= \\sum_{i=1}^N 1 \\cdot (\\mathbf{\\hat{y}}_i - \\mathbf{y}_i) \n",
    "= \\sum_{i=1}^N \\delta_{oi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.591628Z",
     "start_time": "2020-03-14T17:39:23.583680Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_bias_gradient(loss_grad):\n",
    "    \"\"\"Gradients for the output layer bias\"\"\"\n",
    "    return np.sum(loss_grad, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_{h} \n",
    "= \\frac{\\partial J}{\\partial \\mathbf{Z}^h} \n",
    "= \\frac{\\partial J}{\\partial \\mathbf{H}} \\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{Z}^h} \n",
    "= \\frac{\\partial J}{\\partial \\mathbf{Z}^o} \\frac{\\partial \\mathbf{Z}^o}{\\partial H} \\frac{\\partial H}{\\partial \\mathbf{Z}^h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.600468Z",
     "start_time": "2020-03-14T17:39:23.592662Z"
    }
   },
   "outputs": [],
   "source": [
    "def hidden_layer_gradient(H, out_weights, loss_grad):\n",
    "    \"\"\"Error at the hidden layer.\n",
    "    H * (1-H) * (E . Wo^T)\"\"\"\n",
    "    return H * (1 - H) * (loss_grad @ out_weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Weight Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^h} = \\mathbf{X}^T \\cdot \\delta^{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.613410Z",
     "start_time": "2020-03-14T17:39:23.601406Z"
    }
   },
   "outputs": [],
   "source": [
    "def hidden_weight_gradient(X, hidden_layer_grad):\n",
    "    \"\"\"Gradient for the weight parameters at the hidden layer\"\"\"\n",
    "    return X.T @ hidden_layer_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Bias Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T04:24:55.165112Z",
     "start_time": "2019-01-08T04:24:55.161087Z"
    }
   },
   "source": [
    "$$\n",
    "\\frac{\\partial \\xi}{\\partial \\mathbf{b}_{h}} \n",
    "= \\frac{\\partial \\xi}{\\partial H} \\frac{\\partial H}{\\partial Z_{h}} \\frac{\\partial Z_{h}}{\\partial \\mathbf{b}_{h}}\n",
    "= \\sum_{j=1}^N \\delta_{hj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.626532Z",
     "start_time": "2020-03-14T17:39:23.617482Z"
    }
   },
   "outputs": [],
   "source": [
    "def hidden_bias_gradient(hidden_layer_grad):\n",
    "    \"\"\"Gradient for the bias parameters at the output layer\"\"\"\n",
    "    return np.sum(hidden_layer_grad, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.644279Z",
     "start_time": "2020-03-14T17:39:23.630962Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights():\n",
    "    \"\"\"Initialize hidden and output weights and biases\"\"\"\n",
    "\n",
    "    # Initialize hidden layer parameters\n",
    "    hidden_weights = np.random.randn(2, 3)\n",
    "    hidden_bias = np.random.randn(1, 3)\n",
    "\n",
    "    # Initialize output layer parameters\n",
    "    output_weights = np.random.randn(3, 2)\n",
    "    output_bias = np.random.randn(1, 2)\n",
    "    return hidden_weights, hidden_bias, output_weights, output_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:23.660165Z",
     "start_time": "2020-03-14T17:39:23.651110Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradients(X, y_true, w_h, b_h, w_o, b_o):\n",
    "    \"\"\"Evaluate gradients for parameter updates\"\"\"\n",
    "\n",
    "    # Compute hidden and output layer activations\n",
    "    hidden_activations = hidden_layer(X, w_h, b_h)\n",
    "    y_hat = output_layer(hidden_activations, w_o, b_o)\n",
    "\n",
    "    # Compute the output layer gradients\n",
    "    loss_grad = loss_gradient(y_hat, y_true)\n",
    "    out_weight_grad = output_weight_gradient(hidden_activations, loss_grad)\n",
    "    out_bias_grad = output_bias_gradient(loss_grad)\n",
    "\n",
    "    # Compute the hidden layer gradients\n",
    "    hidden_layer_grad = hidden_layer_gradient(hidden_activations, w_o, loss_grad)\n",
    "    hidden_weight_grad = hidden_weight_gradient(X, hidden_layer_grad)\n",
    "    hidden_bias_grad = hidden_bias_gradient(hidden_layer_grad)\n",
    "\n",
    "    return [hidden_weight_grad, hidden_bias_grad, out_weight_grad, out_bias_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to make mistakes with the numerous inputs to the backpropagation algorithm. A simple way to test for accuracy is to compare the change in the output for slightly perturbed parameter values with the change implied by the computed gradient (see [here](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) for more detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:24.058945Z",
     "start_time": "2020-03-14T17:39:23.661626Z"
    }
   },
   "outputs": [],
   "source": [
    "# change individual parameters by +/- eps\n",
    "eps = 1e-4\n",
    "\n",
    "# initialize weights and biases\n",
    "params = initialize_weights()\n",
    "\n",
    "# Get all parameter gradients\n",
    "grad_params = compute_gradients(X, Y, *params)\n",
    "\n",
    "# Check each parameter matrix\n",
    "for i, param in enumerate(params):\n",
    "    # Check each matrix entry\n",
    "    rows, cols = param.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            # change current entry by +/- eps\n",
    "            params_low = deepcopy(params)\n",
    "            params_low[i][row, col] -= eps\n",
    "\n",
    "            params_high = deepcopy(params)\n",
    "            params_high[i][row, col] += eps\n",
    "\n",
    "            # Compute the numerical gradient\n",
    "            loss_high = loss(forward_prop(X, *params_high), Y)\n",
    "            loss_low = loss(forward_prop(X, *params_low), Y)\n",
    "            numerical_gradient = (loss_high - loss_low) / (2 * eps)\n",
    "\n",
    "            backprop_gradient = grad_params[i][row, col]\n",
    "            \n",
    "            # Raise error if numerical and backprop gradient differ\n",
    "            assert np.allclose(numerical_gradient, backprop_gradient), ValueError(\n",
    "                    f'Numerical gradient of {numerical_gradient:.6f} not close to '\n",
    "                    f'backprop gradient of {backprop_gradient:.6f}!')\n",
    "\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:24.063229Z",
     "start_time": "2020-03-14T17:39:24.060105Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_momentum(X, y_true, param_list,\n",
    "                    Ms, momentum_term,\n",
    "                    learning_rate):\n",
    "    \"\"\"Update the momentum matrices.\"\"\"\n",
    "    # param_list = [hidden_weight, hidden_bias, out_weight, out_bias]\n",
    "    # gradients = [hidden_weight_grad, hidden_bias_grad,\n",
    "    #               out_weight_grad, out_bias_grad]\n",
    "    gradients = compute_gradients(X, y_true, *param_list)\n",
    "    return [momentum_term * momentum - learning_rate * grads\n",
    "            for momentum, grads in zip(Ms, gradients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:24.078091Z",
     "start_time": "2020-03-14T17:39:24.064589Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_params(param_list, Ms):\n",
    "    \"\"\"Update the parameters.\"\"\"\n",
    "    # param_list = [Wh, bh, Wo, bo]\n",
    "    # Ms = [MWh, Mbh, MWo, Mbo]\n",
    "    return [P + M for P, M in zip(param_list, Ms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:24.096492Z",
     "start_time": "2020-03-14T17:39:24.079401Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_network(iterations=1000, lr=.01, mf=.1):\n",
    "    # Initialize weights and biases\n",
    "    param_list = list(initialize_weights())\n",
    "\n",
    "    # Momentum Matrices = [MWh, Mbh, MWo, Mbo]\n",
    "    Ms = [np.zeros_like(M) for M in param_list]\n",
    "\n",
    "    train_loss = [loss(forward_prop(X, *param_list), Y)]\n",
    "    for i in range(iterations):\n",
    "        if i % 1000 == 0: print(f'{i:,d}', end=' ', flush=True)\n",
    "        # Update the moments and the parameters\n",
    "        Ms = update_momentum(X, Y, param_list, Ms, mf, lr)\n",
    "\n",
    "        param_list = update_params(param_list, Ms)\n",
    "        train_loss.append(loss(forward_prop(X, *param_list), Y))\n",
    "\n",
    "    return param_list, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:39:24.107100Z",
     "start_time": "2020-03-14T17:39:24.098420Z"
    }
   },
   "outputs": [],
   "source": [
    "# n_iterations = 20000\n",
    "# results = {}\n",
    "# for learning_rate in [.01, .02, .05, .1, .25]:\n",
    "#     for momentum_factor in [0, .01, .05, .1, .5]:\n",
    "#         print(learning_rate, momentum_factor)\n",
    "#         trained_params, train_loss = train_network(iterations=n_iterations, lr=learning_rate, mf=momentum_factor)\n",
    "#         results[(learning_rate, momentum_factor)] = train_loss[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:43.011521Z",
     "start_time": "2020-03-14T17:39:24.108288Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_params, train_loss = train_network(iterations=n_iterations, lr=learning_rate, mf=momentum_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:43.014693Z",
     "start_time": "2020-03-14T17:56:43.012560Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_weights, hidden_bias, output_weights, output_bias = trained_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot displays the training loss over 50K iterations for 50K training samples with a momentum term of 0.5 and a learning rate of 1e-4. \n",
    "\n",
    "It shows that it takes over 5K iterations for the loss to start to decline but then does so very fast. We have not uses stochastic gradient descent, which would have likely significantly accelerated convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:43.697496Z",
     "start_time": "2020-03-14T17:56:43.015661Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = pd.Series(train_loss).plot(figsize=(12, 3), title='Loss per Iteration', xlim=(0, n_iterations), logy=True)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('$\\\\log \\\\xi$', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/ffnn_loss', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots show the function learned by the neural network with a three-dimensional hidden layer form two-dimensional data with two classes that are not linearly separable as shown on the left. The decision boundary misclassifies very few data points and would further improve with continued training. \n",
    "\n",
    "The second plot shows the representation of the input data learned by the hidden layer. The network learns hidden layer weights so that the projection of the input from two to three dimensions enables the linear separation of the two classes. \n",
    "\n",
    "The last plot shows how the output layer implements the linear separation in the form of a cutoff value of 0.5 in the output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:44.651197Z",
     "start_time": "2020-03-14T17:56:43.698925Z"
    }
   },
   "outputs": [],
   "source": [
    "n_vals = 200\n",
    "x1 = np.linspace(-1.5, 1.5, num=n_vals)\n",
    "x2 = np.linspace(-1.5, 1.5, num=n_vals)\n",
    "xx, yy = np.meshgrid(x1, x2)  # create the grid\n",
    "\n",
    "# Initialize and fill the feature space\n",
    "feature_space = np.zeros((n_vals, n_vals))\n",
    "for i in range(n_vals):\n",
    "    for j in range(n_vals):\n",
    "        X_ = np.asarray([xx[i, j], yy[i, j]])\n",
    "        feature_space[i, j] = np.argmax(predict(X_, *trained_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:45.586792Z",
     "start_time": "2020-03-14T17:56:44.652256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a color map to show the classification colors of each grid point\n",
    "cmap = ListedColormap([sns.xkcd_rgb[\"pale red\"],\n",
    "                       sns.xkcd_rgb[\"denim blue\"]])\n",
    "\n",
    "# Plot the classification plane with decision boundary and input samples\n",
    "plt.contourf(xx, yy, feature_space, cmap=cmap, alpha=.25)\n",
    "\n",
    "# Plot both classes on the x1, x2 plane\n",
    "data = pd.DataFrame(X, columns=['$x_1$', '$x_2$']).assign(Class=pd.Series(y).map({0:'negative', 1:'positive'}))\n",
    "sns.scatterplot(x='$x_1$', y='$x_2$', hue='Class', data=data, style=y, markers=['_', '+'], legend=False)\n",
    "plt.title('Decision Boundary')\n",
    "plt.savefig('boundary', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection on Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:45.590876Z",
     "start_time": "2020-03-14T17:56:45.587959Z"
    }
   },
   "outputs": [],
   "source": [
    "n_vals = 25\n",
    "x1 = np.linspace(-1.5, 1.5, num=n_vals)\n",
    "x2 = np.linspace(-1.5, 1.5, num=n_vals)\n",
    "xx, yy = np.meshgrid(x1, x2)  # create the grid\n",
    "X_ = np.array([xx.ravel(), yy.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:46.122374Z",
     "start_time": "2020-03-14T17:56:45.591743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "ax.plot(*hidden_layer(X[y == 0], hidden_weights, hidden_bias).T,\n",
    "        '_', label='negative class', alpha=0.75)\n",
    "ax.plot(*hidden_layer(X[y == 1], hidden_weights, hidden_bias).T,\n",
    "        '+', label='positive class', alpha=0.75)\n",
    "\n",
    "ax.set_xlabel('$h_1$', fontsize=12)\n",
    "ax.set_ylabel('$h_2$', fontsize=12)\n",
    "ax.set_zlabel('$h_3$', fontsize=12)\n",
    "ax.view_init(elev=30, azim=-20)\n",
    "# plt.legend(loc='best')\n",
    "plt.title('Projection of X onto the hidden layer H')\n",
    "plt.tight_layout()\n",
    "plt.savefig('projection3d', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Output Surface Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:46.128272Z",
     "start_time": "2020-03-14T17:56:46.123979Z"
    }
   },
   "outputs": [],
   "source": [
    "zz = forward_prop(X_, hidden_weights, hidden_bias, output_weights, output_bias)[:, 1].reshape(25, -1)\n",
    "zz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T17:56:46.539758Z",
     "start_time": "2020-03-14T17:56:46.129704Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(xx, yy, zz, alpha=.25)\n",
    "ax.set_title('Learned Function')\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_zlabel('$y$', fontsize=12)\n",
    "ax.view_init(elev=45, azim=-20)\n",
    "fig.tight_layout()\n",
    "fig.savefig('surface', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up: we have seen how a very simple network with a single hidden layer with three nodes and a total of 17 parameters is able to learn how to solve a non-linear classification problem using backprop and gradient descent with momentum. \n",
    "\n",
    "We will next review key design choices useful to design and train more complex architectures before we turn to popular deep learning libraries that facilitate the process by providing many of these building blocks and automating the differentiation process to compute the gradients and implement backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
